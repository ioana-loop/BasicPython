{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name:\n",
    "## Collaborators: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "This homework will walk through some of the same steps that we took with HW 1 except we will make the code much more efficient using the numpy library. We will also take a look at methods of model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Load the file 'ptb_data.csv' and 'ptb_test.csv' into two separate arrays and report the dimension, shape and size of both. Then report the mean and standard deviation value of each columns of the the the ptd_data array, not including the last column which will be the labels for the data, then report a list of the unique labels that are in the last column (use the .unique() method). To report the mean and standard deviation make a new array that has three columns, the first column will have the column number, the second will have the mean value of that column, and the third column will have the standard deviation value (ex. [[0,0.1,0.3],[1,0.3,0.4],...,[column_number,mean,standard deviation]]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of dataset_w_labels is: 378000\n",
      "The shape of dataset_w_labels is: (2000, 189)\n",
      "The dimension of dataset_w_labels is: 2\n",
      "The size of testset is: 37600\n",
      "The shape of testset is: (200, 188)\n",
      "The dimension testset is: 2\n",
      "[[0.         0.97678981 0.03369559]\n",
      " [1.         0.7187746  0.19578623]\n",
      " [2.         0.37604515 0.23840843]\n",
      " [3.         0.20613275 0.22568758]\n",
      " [4.         0.16994779 0.19428115]\n",
      " [5.         0.19040655 0.17241055]\n",
      " [6.         0.20306027 0.1620173 ]\n",
      " [7.         0.20672026 0.15947339]\n",
      " [8.         0.20866596 0.15964115]\n",
      " [9.         0.21099684 0.15981231]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 2.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hint for loading in the datasets, we will import pandas first to make it easier:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#uncomment the following lines to import the data into numpy arrays, from now on do not use pandas\n",
    "dataset_w_labels = np.array(pd.read_csv('ptb_data.csv',header = None))\n",
    "testset = np.array(pd.read_csv('ptb_test.csv',header = None))\n",
    "\n",
    "\n",
    "#Should print messages reading 'The size of the dataset is: ...', 'The shape of the testset is: ...', etc. \n",
    "#And should also print the array that contains the means and standard deviations as well\n",
    "\n",
    "print('The size of dataset_w_labels is: ' + str(dataset_w_labels.size))\n",
    "print('The shape of dataset_w_labels is: ' + str(dataset_w_labels.shape))\n",
    "print('The dimension of dataset_w_labels is: ' + str(dataset_w_labels.ndim))\n",
    "\n",
    "print('The size of testset is: ' + str(testset.size))\n",
    "print('The shape of testset is: ' + str(testset.shape))\n",
    "print('The dimension testset is: ' + str(testset.ndim))\n",
    "\n",
    "mean_std = np.empty([dataset_w_labels.shape[1]-1,3])\n",
    "for i in range(dataset_w_labels.shape[1]-1):\n",
    "    mean_std[i,:] = np.array([i,np.mean(dataset_w_labels[:,i]),np.std(dataset_w_labels[:,i])])\n",
    "\n",
    "print(mean_std[:10,:])\n",
    "np.unique(dataset_w_labels[:,188])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "\n",
    "We are going to split the dataset into two separate arrays to make calculations easier. Make a new array that only contains the labels from the dataset and make a new array that contains only the data from the dataset (so all the columns except for the last one).\n",
    "\n",
    "Now take the first row of the dateset and the first row of the testset and subtract them, then square the result, then sum that new array, and then take the squareroot of that result. You should end with a number, which will actually be the distance between those two points (note every row is a point in this array).\n",
    "\n",
    "To make sure that makes sense here is vector subtraction works:\n",
    "\n",
    "$x = (x_1,x_2,...,x_n), \\; y = (y_1,y_2,...,y_n)$ and so $x-y = (x_1-y_1,x_2-y_2,..., x_n-y_n)$\n",
    "And if you have had linear algebra before we want the magnitude of that difference, so:\n",
    "$|x-y| = \\sqrt{\\sum_{j=1}^n (x_j-y_j)^2} = \\sqrt{(x_1-y_1)^2 + ... + (x_n-y_n)^2}$ which is exactly our distance function from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2542169395339067\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_w_labels[:,:-1]\n",
    "labels = dataset_w_labels[:,-1]\n",
    "\n",
    "x = dataset[0,:]\n",
    "y = testset[0,:]\n",
    "\n",
    "z = x-y\n",
    "z = z**2\n",
    "z = np.sum(z)\n",
    "z = z**0.5\n",
    "\n",
    "\n",
    "#should print out a number at the end\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "We will look at how we can make use of broadcasting in numpy to calculate distances super fast. Uncomment the two lines of code below and notice the shape of each array. One is a 4x4 array while the other is 1x4, now lets see what happens if we subtract them. Notice that it is as if the smaller array was subtracted from each row of the larger 4x4.\n",
    "Now take the difference between the entire dataset and the first row of the testset and describe (in a comment) what the result of this operation will give us. Now square the result, sum each row, and take the squareroot of the result. We should be left with a one dimensional array, describe (in a comment) what this array represents and why (it has to do with distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-19   0   0  -6]\n",
      " [-10   0  27   0]\n",
      " [-18   2   2  -9]\n",
      " [-10  18  27  30]]\n",
      "[2.25421694 3.57338808 2.29290585 ... 2.50079379 3.20609083 2.54303461]\n"
     ]
    }
   ],
   "source": [
    "four_by_four = np.array([[1,2,3,4],[10,2,30,10],[2,4,5,1],[10,20,30,40]])\n",
    "one_by_four = np.array([20,2,3,10])\n",
    "print(four_by_four - one_by_four)\n",
    "\n",
    "dist = np.sum((dataset - testset[0,:])**2,axis = 1)**0.5\n",
    "\n",
    "print(dist)\n",
    "\n",
    "#Leave a comment of what the dataset - one row of the testset will be\n",
    "##dataset - testset[0,:] will subtract that single row of the testset from each row of the dataset\n",
    "\n",
    "#Leave a comment of what the array of values represents.\n",
    "##The ith element in this array is the distance between the ith row in the dataset and the first row of the testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "In Question 3 we have seen an easy way to compute the distance between each point in a dataset and a testpoint. Now we have an array of distances and want to find the closest ones, so we need a way to sort. Instead of using .sort() we are going to use .argsort(). \n",
    "\n",
    "First uncomment the array in the cell below and use .argsort() on it, and print the result of that? What does it represent? Now take to_sort[to_sort.argsort()] and print that result, and describe what it represents. Argsort gives us the indices of the sorted list. So take the array of distances from question 3 and use argsort on it, and print a slice of the first 5 entries in in the argsort output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3 0 4 6 5]\n",
      "[ 824  808  260  292  169  289  203  999  257 1955]\n"
     ]
    }
   ],
   "source": [
    "to_sort = np.array([40,20,10,30,50,70,60])\n",
    "print(to_sort.argsort())\n",
    "\n",
    "#describe in a comment what the argsort function returns\n",
    "##argsort returns an array that holds the indices of the values after sorting. In the example, to_sort[2] should be the\n",
    "##first element in a sorted array, so 2 should be the first entry in to_sort.arg_sort()\n",
    "\n",
    "#print the first 10 entries of the .argsort() output from the distances found in question 3\n",
    "\n",
    "print(dist.argsort()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "So we have a way to find the indices of the 5 closest points to a testpoint, now realize we can use the .argsort() result on any array of the same length, namely the labels array. store the array labels[distances.argsort()] into a new variable and describe what it represents in a comment. Then return the mode of the first ten labels from this variable, and describe in a comment what this mode is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import statistics as st\n",
    "print(labels[dist.argsort()][:10])\n",
    "print(st.mode(labels[dist.argsort()][:10]))\n",
    "#should print out the mode of the ten closest labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Realize what we have just done! We have taken the distance between every point in the dataset with a single testpoint, sorted those distances and returned the most common label of the 5 closest points! This is the k-nearest algorithm that we were making in HW 1, but now a new and improved version with numpy. So, create a function that takes in four inputs, an array of points that will be the dataset, an array of points that are the labels of the dataset array,an array of testpoints, and lastly a parameter k. The function should return an array of the predicted labels for each of the testpoints in the test set array.\n",
    "\n",
    "To be clear, for each row in the testset you are going to store the most common label of the k-closest points to the testpoint, so you will have to iterate over each row of the testset to repeat this process. At the end, please test your function on the ptb dataset and ptb testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statistics as st\n",
    "import pandas as pd\n",
    "\n",
    "def k_nearest(dataset,labels,testset,k):\n",
    "    \n",
    "    #Create an array that will store my predicted labels\n",
    "    predicted_labels = np.empty([testset.shape[0]])\n",
    "    \n",
    "    #iterate through each row in the testset, note that the variable 'row' will be a number\n",
    "    for row in range(testset.shape[0]):\n",
    "        \n",
    "        #take the current row and make it its own variable, then find the distances between the dataset and the \n",
    "        #current row like we did in question 3\n",
    "        curr_row = testset[row,:]\n",
    "        distances = np.sum((dataset - curr_row)**2,axis = 1)**0.5\n",
    "        \n",
    "        #now get the k closest labels using the argsort function\n",
    "        k_closest_labels = labels[distances.argsort()][:k]\n",
    "        \n",
    "        #store the mode in the appropriate entry in predicted labels aray\n",
    "        predicted_labels[row] = st.mode(k_closest_labels)\n",
    "    return predicted_labels\n",
    "\n",
    "print(k_nearest(dataset,labels,testset,5))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Using the random module from numpy, set np.random.seed(5) and use the np.random.choice() function to select 100 random (and unique) rows from the original ptb dataset (this includes the labels). These 100 random rows will be our new test set, so test your k_nearest function using the ptd dataset (with the random rows removed from it) and the new random testset that we made. This should return an array of predicted labels for the testset, but the testset already has labels associated with it, so report the number of times that our predicted labels did not match the real labels.\n",
    "\n",
    "Note, remove the labels from the test set and store them in a new test_labels array to use for comparison later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "\n",
    "#create array that has the random indexes\n",
    "rand_index= np.random.choice(dataset.shape[0],100,replace=False)\n",
    "\n",
    "#create new testset and testset labels \n",
    "new_test_set = dataset_w_labels[rand_index,:-1]\n",
    "new_test_set_labels = dataset_w_labels[rand_index,-1]\n",
    "\n",
    "#delete those rows from the dataset\n",
    "reduced_dataset = np.delete(dataset,rand_index,0)\n",
    "reduced_labels = np.delete(labels,rand_index,0)\n",
    "\n",
    "#run our algorithm with the new data\n",
    "pred_val = k_nearest(reduced_dataset,reduced_labels,new_test_set,3)\n",
    "\n",
    "\n",
    "print(100-sum(pred_val == new_test_set_labels))\n",
    "\n",
    "#remove the rows according to rand_index, and make a new testset and test_labels array \n",
    "#then use your k_nearest function to get the predicted labels for the testset and compare them to the true labels\n",
    "\n",
    "#print the number of times your predicted labels do not match the true labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Repeat the process of question 7 except this time reduce your dataset to the columns numbered 0,101,102,103,106,122,123,188. So you should have a dataset that only has 8 columns now. Again, report the number of times your predicted labels disagree with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "\n",
    "#same thing as before\n",
    "rand_index= np.random.choice(dataset.shape[0],100,replace=False)\n",
    "new_test_set = dataset_w_labels[rand_index,:-1]\n",
    "new_test_set_labels = dataset_w_labels[rand_index,-1]\n",
    "reduced_dataset = np.delete(dataset,rand_index,0)\n",
    "reduced_labels = np.delete(labels,rand_index,0)\n",
    "\n",
    "#now only take the specified columns\n",
    "new_columns = [0,101,102,103,106,122,123]\n",
    "new_test_set = new_test_set[:,new_columns]\n",
    "reduced_dataset = reduced_dataset[:,new_columns]\n",
    "\n",
    "#run our algorithm \n",
    "pred_val = k_nearest(reduced_dataset,reduced_labels,new_test_set,3)\n",
    "\n",
    "#print results\n",
    "print(100-sum(pred_val == new_test_set_labels))\n",
    "\n",
    "#print the number of times your predicted labels do not match the true labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Update your k_nearest so that it takes in a four inputs, a filename for the dataset (labels are assumed to be the last column), a filename for the testset, a filename for the output, and the parameter k. Your new function will open the files inside of the function and then write the results to a new file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It would be useful to look up how to write arrays to csv with numpy or pandas.\n",
    "import statistics as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def k_nearest_from_files(data_filename,test_filename,out_filename,k):\n",
    "    dataset_w_labels = np.array(pd.read_csv(data_filename,header = None))\n",
    "    testset = np.array(pd.read_csv(test_filename,header = None))\n",
    "\n",
    "    dataset = dataset_w_labels[:,:-1]\n",
    "    labels = dataset_w_labels[:,-1]\n",
    "    \n",
    "    pred_labels = np.empty([testset.shape[0],1])\n",
    "    for i in range(testset.shape[0]):\n",
    "        curr_row = testset[i,:]\n",
    "\n",
    "        distances = np.sum((dataset-curr_row)**2,axis = 1)**0.5\n",
    "        pred_label = stats.mode(labels[distances.argsort()][:k])\n",
    "        pred_labels[i] = pred_label\n",
    "    testset = np.concatenate([testset,pred_labels.reshape(pred_labels.shape[0],1)],axis = 1)\n",
    "    pd.DataFrame(testset).to_csv(out_filename,header = None)\n",
    "\n",
    "k_nearest_from_files('ptb_data.csv','ptb_test.csv','out.csv',3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
